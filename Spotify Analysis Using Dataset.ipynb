{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.cache_handler import CacheFileHandler\n",
    "\n",
    "spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id=os.environ['SPOTIFY_CLIENT_ID'], client_secret=os.environ['SPOTIFY_CLIENT_SECRET'], cache_handler=CacheFileHandler(username='keatonconrad')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "full_df = pd.read_csv('./complete_project_data.csv')\n",
    "print(full_df.columns)\n",
    "full_df.describe()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hit_df = full_df.loc[full_df['Label'] == 1]\n",
    "hit_df.describe()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "no_hit_df = full_df.loc[full_df['Label'] == 0]\n",
    "no_hit_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for column in full_df.columns:\n",
    "    try:\n",
    "        t = ttest_ind(hit_df[column].astype(float), no_hit_df[column].astype(float))\n",
    "        print(column + ' - T: ' + str(t[0]) + ', p: ' + str(t[1]))\n",
    "    except ValueError:\n",
    "        continue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = full_df.head(500)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import lyricsgenius\n",
    "genius = lyricsgenius.Genius('rRYZ3FEWX4MeR22Nhk40IBkl8y8OB2PrybJ-R_rgeBTcEusrWdF5EDtm7L33-3Qc')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import re\n",
    "from textblob import TextBlob\n",
    "\n",
    "polarity = []\n",
    "subjectivity = []\n",
    "lyrics = []\n",
    "\n",
    "for i, song in df.iterrows():\n",
    "    try:\n",
    "        song = genius.search_song(song['Track'], song['Artist'])\n",
    "        song_lyrics = re.sub(r'\\[.*?\\]\\n', '', song.lyrics).replace('\\n', ' ')\n",
    "        blob = TextBlob(song_lyrics)\n",
    "        polarity.append(blob.sentiment[0])\n",
    "        subjectivity.append(blob.sentiment[1])\n",
    "        lyrics.append(song_lyrics)\n",
    "    except:\n",
    "        polarity.append(None)\n",
    "        subjectivity.append(None)\n",
    "        lyrics.append(None)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = pd.DataFrame(df)\n",
    "df['Polarity'] = polarity\n",
    "df['Subjectivity'] = subjectivity\n",
    "df['Lyrics'] = lyrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df.dropna(inplace=True)\n",
    "df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_data = df['Label']\n",
    "x_data = df.drop(['Label', 'Artist', 'Track', 'Year', 'Month', 'ArtistScore', 'Lyrics'], axis=1, inplace=False)\n",
    "x_data_train, x_data_test, y_data_train, y_data_test = train_test_split(\n",
    "    x_data, y_data, test_size=0.15, shuffle=False\n",
    ")\n",
    "print(len(y_data_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler_model = MinMaxScaler()\n",
    "scaler_model.fit(x_data_train)\n",
    "\n",
    "\n",
    "x_data_train = pd.DataFrame(\n",
    "    scaler_model.transform(x_data_train),\n",
    "    columns=x_data_train.columns,\n",
    "    index=x_data_train.index\n",
    ")\n",
    "x_data_test = pd.DataFrame(\n",
    "    scaler_model.transform(x_data_test),\n",
    "    columns=x_data_test.columns,\n",
    "    index=x_data_test.index\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(random_state=0).fit(x_data_train, y_data_train)\n",
    "round(LR.score(x_data_test, y_data_test), 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "SVM = sk.svm.LinearSVC()\n",
    "SVM.fit(x_data_train, y_data_train)\n",
    "round(SVM.score(x_data_test, y_data_test), 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=100, max_depth=2, random_state=0)\n",
    "RF.fit(x_data_train, y_data_train)\n",
    "round(RF.score(x_data_test, y_data_test), 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier(solver='lbfgs', alpha=1e-1, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "NN.fit(x_data_train, y_data_train)\n",
    "round(NN.score(x_data_test, y_data_test), 4)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "\n",
    "    def __init__(self, max_sequence_len=50, texts=None, filename=None,\n",
    "                 embedding_s3_bucket=None, embedding_s3_key=None, num_words=None,\n",
    "                 char_level=False):\n",
    "        self.word_index = {}\n",
    "        self.embeddings_index = {}\n",
    "        self.embedding_matrix = {}\n",
    "        self.vocab_size = 0\n",
    "        self.embedding_dimension = 0\n",
    "        self.max_sequence_len = max_sequence_len\n",
    "        self.tokenizer = Tokenizer(\n",
    "            num_words=num_words,\n",
    "            filters='\\t\\n',\n",
    "            char_level=char_level,\n",
    "            lower=False,\n",
    "            oov_token='<unknown>'  # Sets words it doesn't know to this value\n",
    "        )\n",
    "        if texts is not None:\n",
    "            self.generate_word_index(texts)\n",
    "        if filename or (embedding_s3_bucket and embedding_s3_key):\n",
    "            self.load_pretrained_embedding(filename, embedding_s3_bucket,\n",
    "                                           embedding_s3_key)\n",
    "            if texts is not None:\n",
    "                self.generate_embedding_matrix()\n",
    "\n",
    "    def generate_word_index(self, texts):\n",
    "        \"\"\"\n",
    "        Creates the word index from the given texts.\n",
    "\n",
    "        Args:\n",
    "            texts: Array of strings\n",
    "        Returns:\n",
    "        The generated word index\n",
    "        \"\"\"\n",
    "\n",
    "        self.tokenizer.fit_on_texts(texts)\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "        self.vocab_size = len(self.word_index) + 1\n",
    "        return self.word_index\n",
    "\n",
    "    def generate_sequences(self, texts):\n",
    "        \"\"\"\n",
    "        Transforms texts into sequences of word indices.\n",
    "        Pads sequences so that they have equal length.\n",
    "        Only callable after word index has been generated.\n",
    "\n",
    "        Args:\n",
    "            texts: Array of strings\n",
    "        Returns:\n",
    "            The padded sequences as a 2D-array of strings\n",
    "        \"\"\"\n",
    "\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        padded = pad_sequences(sequences, maxlen=self.max_sequence_len,\n",
    "                               padding='post', truncating='post')\n",
    "        return padded\n",
    "\n",
    "    def load_pretrained_embedding(self, filename=None, s3_bucket=None, s3_key=None):\n",
    "        \"\"\"\n",
    "        Loads a pretrained embeddings index from the given file.\n",
    "        Assumes a file with one line per embedding, starting with word and\n",
    "        followed by coefficients (separated by spaces).\n",
    "\n",
    "        Args:\n",
    "            filename: Path to embedding\n",
    "            s3_bucket: S3 Bucket where embedding is stored\n",
    "            s3_key: Key of embedding file stored on S3\n",
    "        Returns:\n",
    "            The loaded embeddings index\n",
    "        \"\"\"\n",
    "\n",
    "        coefs = []\n",
    "\n",
    "        if filename:\n",
    "            with open(filename, encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    values = line.split()\n",
    "                    word = values[0]\n",
    "                    coefs = np.asarray(values[1:], dtype='float32')\n",
    "                    self.embeddings_index[word] = coefs\n",
    "                self.embedding_dimension = len(coefs)\n",
    "            return self.embeddings_index\n",
    "\n",
    "        else:\n",
    "            obj = s3.Object(s3_bucket, s3_key)\n",
    "            for line in obj.get()['Body'].iter_lines():\n",
    "                values = line.decode('utf-8').split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                self.embeddings_index[word] = coefs\n",
    "            self.embedding_dimension = len(coefs)\n",
    "            return self.embeddings_index\n",
    "\n",
    "    def generate_embedding_matrix(self):\n",
    "        \"\"\"\n",
    "        Creates embedding matrix from word and embeddings indices.\n",
    "\n",
    "        Returns:\n",
    "            Generated embedding matrix as 2D numpy array\n",
    "        \"\"\"\n",
    "\n",
    "        embedding_matrix = np.zeros((len(self.word_index) + 1, self.embedding_dimension))\n",
    "        for word, i in self.word_index.items():\n",
    "            embedding_vector = self.embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                # Words not found will stay all-zeros\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        return self.embedding_matrix\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.9.4\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de8fc500061d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('Building model...')\n",
    "# Branch 1\n",
    "aux_input = Input(shape=(len(X_train.columns),))\n",
    "aux = Dense(32, activation=\"relu\")(aux_input)\n",
    "aux = BatchNormalization()(aux)\n",
    "aux = Model(inputs=aux_input, outputs=aux)\n",
    "\n",
    "# Branch 2\n",
    "emb_input = Input(shape=(embedding.max_sequence_len,))\n",
    "emb = Embedding(embedding.vocab_size, embedding.embedding_dimension,\n",
    "                input_length=embedding.max_sequence_len,\n",
    "                weights=[embedding.embedding_matrix], trainable=True)(emb_input)\n",
    "emb = Conv1D(filters=32, kernel_size=2, activation='relu')(emb)\n",
    "emb = MaxPooling1D(4)(emb)\n",
    "emb = Flatten()(emb)\n",
    "emb = BatchNormalization()(emb)\n",
    "emb = Model(inputs=emb_input, outputs=emb)\n",
    "\n",
    "# Combined\n",
    "combined = concatenate([aux.output, emb.output])\n",
    "z = Dense(32, activation=\"relu\")(combined)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(.1)(z)\n",
    "z = Dense(32, activation=\"relu\")(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(.1)(z)\n",
    "z = Dense(1, activation=\"relu\")(z)\n",
    "\n",
    "model = Model(inputs=[aux.input, emb.input], outputs=z)\n",
    "\n",
    "optimizer = keras.optimizers.Adam(.05)\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=5, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=.2, patience=5, min_lr=0.001)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(seqs_train.shape)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train, seqs_train],\n",
    "    y_train,\n",
    "    validation_data=([X_eval, seqs_eval], y_eval),\n",
    "    batch_size=512,\n",
    "    epochs=1,\n",
    "    shuffle=True,\n",
    "    callbacks=[es, rlr]\n",
    ")\n",
    "\n",
    "pred_train = model.evaluate([X_train, seqs_train], y_train)\n",
    "pred_test = model.evaluate([X_eval, seqs_eval], y_eval)\n",
    "y_pred = model.predict([X_eval, seqs_eval]).astype('int').flatten()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}