{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "%pip install spotipy --upgrade\n",
    "%pip install lyricsgenius\n",
    "%pip install textblob"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting spotipy\n",
      "  Using cached spotipy-2.18.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.7/site-packages (from spotipy) (1.15.0)\n",
      "Collecting urllib3>=1.26.0\n",
      "  Using cached urllib3-1.26.4-py2.py3-none-any.whl (153 kB)\n",
      "Collecting requests>=2.25.0\n",
      "  Using cached requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.25.0->spotipy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.25.0->spotipy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.25.0->spotipy) (2020.12.5)\n",
      "Installing collected packages: urllib3, requests, spotipy\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.11\n",
      "    Uninstalling urllib3-1.25.11:\n",
      "      Successfully uninstalled urllib3-1.25.11\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "Successfully installed requests-2.25.1 spotipy-2.18.0 urllib3-1.26.4\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting lyricsgenius\n",
      "  Using cached lyricsgenius-3.0.1-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/site-packages (from lyricsgenius) (2.25.1)\n",
      "Collecting beautifulsoup4>=4.6.0\n",
      "  Using cached beautifulsoup4-4.9.3-py3-none-any.whl (115 kB)\n",
      "Collecting soupsieve>1.2\n",
      "  Using cached soupsieve-2.2.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests>=2.20.0->lyricsgenius) (1.26.4)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, lyricsgenius\n",
      "Successfully installed beautifulsoup4-4.9.3 lyricsgenius-3.0.1 soupsieve-2.2.1\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.15.3-py2.py3-none-any.whl (636 kB)\n",
      "Collecting nltk>=3.1\n",
      "  Using cached nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "Collecting click\n",
      "  Using cached click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.0.1)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "Collecting regex\n",
      "  Using cached regex-2021.4.4-cp37-cp37m-manylinux2014_x86_64.whl (720 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk, textblob\n",
      "Successfully installed click-7.1.2 nltk-3.6.2 regex-2021.4.4 textblob-0.15.3 tqdm-4.60.0\n",
      "\u001b[33mWARNING: You are using pip version 21.0.1; however, version 21.1 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from spotipy.cache_handler import CacheFileHandler\n",
    "\n",
    "spotify = spotipy.Spotify(client_credentials_manager=SpotifyClientCredentials(client_id=os.environ['SPOTIFY_CLIENT_ID'], client_secret=os.environ['SPOTIFY_CLIENT_SECRET'], cache_handler=CacheFileHandler(username='keatonconrad')))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Importing Data\n",
    "\n",
    "## Million Song Dataset\n",
    "\n",
    "To start, we import a subset of the [Million Song Dataset](http://millionsongdataset.com/) and read it into a DataFrame with pandas for easy processing. The dataset contains random songs of various genres from 1922 to 2011. We'll use this to collect non-hit songs.\n",
    "\n",
    "The DataFrame is shuffled to mitigate any bias in the order."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "song_data = pd.read_csv('./song-list.txt', sep='<SEP>', engine='python')\n",
    "song_data = song_data.sample(frac=1)\n",
    "song_data.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Spotify and Billboard Data\n",
    "\n",
    "We import a dataset that contains historical Billboard Hot 100 data mapped to each song's Spotify ID. This is incredibly useful as it helps us collect the audio features later. This dataset will be used to collect features from hit songs.\n",
    "\n",
    "Like the previous dataset, we shuffle it to avoid any ordering bias."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "spotify_billboard_data = pd.read_csv('./spotify-billboard-data.csv')\n",
    "spotify_billboard_data = spotify_billboard_data.sample(frac=1)\n",
    "print(spotify_billboard_data.columns)\n",
    "hit_track_ids = spotify_billboard_data['spotify_track_id'].tolist()\n",
    "spotify_billboard_data.describe()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Enrichment"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def get_track_info(track, hit):\n",
    "    # Helper function to clean up code later\n",
    "    \n",
    "    explicit = 1 if track['explicit'] else 0\n",
    "    try:\n",
    "        isrc = track['external_ids']['isrc']\n",
    "    except KeyError:\n",
    "        isrc = None\n",
    "        \n",
    "    return {\n",
    "        'spotify_track_id': track['id'],\n",
    "        'isrc': isrc,\n",
    "        'artist': track['artists'][0]['name'],\n",
    "        'artist_id': track['artists'][0]['id'],\n",
    "        'song': track['name'],\n",
    "        'spotify_track_album': track['album']['name'],\n",
    "        'year': track['album']['release_date'][:4],\n",
    "        'explicit': explicit,\n",
    "        'hit': hit,\n",
    "        'current_popularity': track['popularity'] # As of 4/20/21\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Non-Hit Songs\n",
    "\n",
    "We can't do much with just the song title and artist, as given in the Million Song Dataset. Here, we do a search on Spotify to find the closest matching song and retrieve the track ID, album, and explicit information, among other data.\n",
    "\n",
    "It should be noted that this step has some inherent inaccuracy due to its reliance on Spotify's search feature. There is no guarantee the correct song will be returned."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "no_hit_arr = []\n",
    "song_data_sample = song_data.head(len(spotify_billboard_data))\n",
    "\n",
    "for i, song in tqdm(song_data_sample.iterrows(), total=song_data_sample.shape[0]):\n",
    "    artist = song['Performer'].replace('Featuring ', '').replace('feat. ', '').replace('feat ', '').replace('& ', '')\n",
    "    search_results = spotify.search(q=song['Song'] + ' ' + artist, limit=1, type='track', market='US')\n",
    "    try:\n",
    "        track = search_results['tracks']['items'][0]\n",
    "    except IndexError: # If search didn't return anything\n",
    "        continue\n",
    "        \n",
    "    if track['id'] not in hit_track_ids:\n",
    "        no_hit_arr.append(get_track_info(track, hit=0))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is consolidated into a DataFrame for ease of processing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "no_hit_basic_song_data_df = pd.DataFrame(no_hit_arr)\n",
    "no_hit_basic_song_data_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hit Songs\n",
    "\n",
    "While the Spotify and Billboard dataset provides more data than the Million Song Dataset we are using, we still collect more information from Spotify about the album and release year.\n",
    "\n",
    "This step isn't necessarily required, as the album name isn't used for much. However, it is quick due to already having the track IDs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hit_arr = []\n",
    "track_ids = []\n",
    "\n",
    "for i, song in tqdm(spotify_billboard_data.iterrows(), total=spotify_billboard_data.shape[0]):\n",
    "    track_ids.append(song['spotify_track_id'])\n",
    "    \n",
    "    if len(track_ids) == 50 or i == spotify_billboard_data.shape[0]:\n",
    "        results = spotify.tracks(tracks=track_ids)['tracks']\n",
    "        for track in results:            \n",
    "            hit_arr.append(get_track_info(track, hit=1))\n",
    "        track_ids.clear()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As before, the data is turned into a DataFrame."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hit_basic_song_data_df = pd.DataFrame(hit_arr)\n",
    "hit_basic_song_data_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have basic information (and more importantly, the Spotify track IDs) from our dataset, we concatenate both the non-hits and the hits together into one larger dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "basic_song_data_df = pd.concat([no_hit_basic_song_data_df, hit_basic_song_data_df])\n",
    "basic_song_data_df = basic_song_data_df.sample(frac=1)\n",
    "basic_song_data_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Collection\n",
    "\n",
    "We collect the audio features for each track using the Spotify API and the track IDs from the newly created DataFrame.\n",
    "\n",
    "Due to how the code makes API calls in batches of 100, it will ignore the last few tracks."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features = []\n",
    "track_ids = []\n",
    "\n",
    "for track_id in tqdm(basic_song_data_df['spotify_track_id']):\n",
    "    track_ids.append(track_id)\n",
    "    if len(track_ids) == 100:\n",
    "        results = spotify.audio_features(tracks=track_ids)\n",
    "        for result in results:\n",
    "            if result is None:\n",
    "                # Sometimes it returns None, this sets a default so we can still add it to a df\n",
    "                features.append({\n",
    "                    'danceability': None,\n",
    "                    'energy': None,\n",
    "                    'key': None,\n",
    "                    'loudness': None,\n",
    "                    'mode': None,\n",
    "                    'speechiness': None,\n",
    "                    'acousticness': None,\n",
    "                    'instrumentalness': None,\n",
    "                    'liveness': None,\n",
    "                    'valence': None,\n",
    "                    'tempo': None,\n",
    "                    'type': None,\n",
    "                    'id': None,\n",
    "                    'uri': None,\n",
    "                    'track_href': None,\n",
    "                    'analysis_url': None,\n",
    "                    'duration_ms': None,\n",
    "                    'time_signature': None\n",
    "                })\n",
    "            else:\n",
    "                features.append(result)\n",
    "        track_ids.clear()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features_df = pd.DataFrame(features)\n",
    "features_df.describe()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to the rounding-off of the last few songs as mentioned above, we ensure both the `basic_song_data_df` and the `features_df` are of equal length.\n",
    "\n",
    "Finally, we concatenate them, forming one DataFrame with basic song information and the audio features of each track."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(len(basic_song_data_df))\n",
    "print(len(features_df))\n",
    "\n",
    "# features_df.drop(features_df.tail(24).index, inplace=True)\n",
    "\n",
    "print(len(basic_song_data_df))\n",
    "print(len(features_df))\n",
    "\n",
    "basic_data = basic_song_data_df.reset_index(drop=True, inplace=False)\n",
    "feature_data = features_df.reset_index(drop=True, inplace=False)\n",
    "\n",
    "full_df = pd.concat([basic_data, feature_data], axis=1)\n",
    "full_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "\"\"\"\n",
    "for column in full_df.columns:\n",
    "    try:\n",
    "        t = ttest_ind(hit_df[column].astype(float), no_hit_df[column].astype(float))\n",
    "        print(column + ' - T: ' + str(t[0]) + ', p: ' + str(t[1]))\n",
    "    except ValueError:\n",
    "        continue\n",
    "\"\"\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For sanity's sake, we save the DataFrame for easy loading later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "\n",
    "df = full_df\n",
    "with open('all_song_data_new.pickle', 'wb') as f:\n",
    "    pickle.dump(pd.DataFrame(df), f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('all_song_data_new.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lyric Collection\n",
    "\n",
    "Using the Genius API, we collect the lyrics of every song in our dataset. The polarity and subjectivity of the lyrics are also saved to an array.\n",
    "\n",
    "Similar to above, this step has inherent inaccuracies due to its reliance on Genius's search feature.\n",
    "\n",
    "The polariy, subjectivity, and lyrics are pickled and saved for easy loading later."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import lyricsgenius\n",
    "genius = lyricsgenius.Genius(os.environ['GENIUS_TOKEN'], verbose=False, remove_section_headers=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "lyrics = []\n",
    "\n",
    "for i, song in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    try:\n",
    "        song = genius.search_song(song['song'], song['artist'], get_full_info=False)\n",
    "        song_lyrics = song.lyrics.replace('\\n', ' ')\n",
    "        lyrics.append(song_lyrics)\n",
    "    except:\n",
    "        lyrics.append(None)\n",
    "        \n",
    "with open('lyrics.pickle', 'wb') as f:\n",
    "    pickle.dump(lyrics, f)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#%pip install demoji\n",
    "import pickle\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('lyrics.pickle', 'rb') as f:\n",
    "    lyrics = pickle.load(f)\n",
    "\n",
    "# lyrics = [utils.strip_stop_words(lyric) for lyric in tqdm(lyrics)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "polarity = []\n",
    "subjectivity = []\n",
    "\n",
    "for lyric in tqdm(lyrics):\n",
    "    try:\n",
    "        blob = TextBlob(lyric)\n",
    "        polarity.append(blob.sentiment[0])\n",
    "        subjectivity.append(blob.sentiment[1])\n",
    "    except:\n",
    "        polarity.append(None)\n",
    "        subjectivity.append(None)\n",
    "\n",
    "with open('polarity.pickle', 'wb') as f:\n",
    "    pickle.dump(polarity, f)\n",
    "with open('subjectivity.pickle', 'wb') as f:\n",
    "    pickle.dump(subjectivity, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "\n",
    "with open('polarity.pickle', 'rb') as f:\n",
    "    polarity = pickle.load(f)\n",
    "with open('subjectivity.pickle', 'rb') as f:\n",
    "    subjectivity = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Artist Popularity\n",
    "\n",
    "This gets the popularity of every artist"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "popularities = []\n",
    "artist_ids = []\n",
    "\n",
    "for i, artist_id in tqdm(enumerate(df['artist_id']), total=len(df['artist_id'])):\n",
    "    artist_ids.append(artist_id)\n",
    "    if len(artist_ids) == 50 or i == len(df[['artist_id']]):\n",
    "        results = spotify.artists(artist_ids)\n",
    "        for result in results['artists']:\n",
    "            popularities.append(result['popularity'])\n",
    "        artist_ids.clear()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_backup = df.copy()\n",
    "df.drop(df.tail(26).index, inplace=True)\n",
    "df['artist_popularity'] = popularities"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Previous Artist Hits\n",
    "\n",
    "This gets an artist's previous hits. It adds up all the artist's hit songs in the dataset that are in the same year or previous years, not counting that song, if it's a hit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_hits = []\n",
    "\n",
    "# NOTE: This is an incredibly inefficient way to do this\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    subset_df = df[df['year'] <= row['year']]\n",
    "    artist_hits = subset_df.groupby(['artist_id'])['hit'].agg('sum').reset_index()\n",
    "    hits = artist_hits.loc[artist_hits.artist_id==row['artist_id'], 'hit'].values[0]\n",
    "    if hits > 0 and row['hit'] == 1: # Don't count this song if it's a hit\n",
    "        hits-=1\n",
    "    num_hits.append(hits)\n",
    "    \n",
    "df['artist_num_hits'] = num_hits"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The polarity, subjectivity, and lyric arrays are added to our dataset as columns. Rows with incomplete data are dropped."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df['polarity'] = polarity[:-26]\n",
    "df['subjectivity'] = subjectivity[:-26]\n",
    "df['lyrics'] = lyrics[:-26]\n",
    "df['lyric_length'] = df['lyrics'].str.len()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dupl_df = df[df.duplicated(subset='id')].sort_values(by='id', axis=0)\n",
    "print(len(set(df['id'].tolist())))\n",
    "print(len(df['id'].tolist()))\n",
    "df.drop_duplicates(subset='id', inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('all_data.pickle', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_data.pickle', 'rb') as f:\n",
    "    df = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Audio Features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "\n",
    "for i, song in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        if song['id'] is None:\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            track = spotify.tracks(tracks=[song['id']])['tracks'][0]\n",
    "\n",
    "            doc = requests.get(track['preview_url'])\n",
    "            with open('./songs/' + song['id'] + '.mp3', 'wb') as f:\n",
    "                f.write(doc.content)\n",
    "        except:\n",
    "            with open('./songs/' + song['id'] + '.mp3', 'wb') as f:\n",
    "                f.write(b'')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('./00s8dO3RWrFkBqC9JIy6ag.mp3', 'rb') as f:\n",
    "    print(f)\n",
    "    fs, song_data = audioBasicIO.read_audio_generic(f)\n",
    "\n",
    "print(fs)\n",
    "print(song_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "df['lyric_length'] = df['lyric_length'].fillna(0)\n",
    "df['lyrics'] = df['lyrics'].fillna('')\n",
    "df.dropna(subset=['danceability'], inplace=True) # Drops all rows that don't have basic features for whatever reason"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "print(len(df))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "41909\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "with open('./audio_features/zero_crossings.pickle', 'rb') as f:\n",
    "    zero_crossings = pickle.load(f)\n",
    "    \n",
    "df['zero_crossings'] = zero_crossings"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "with open('./audio_features/mels.pickle', 'rb') as f:\n",
    "    mels = pickle.load(f)\n",
    "    \n",
    "df['mels'] = mels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "with open('./audio_features/mfs.pickle', 'rb') as f:\n",
    "    mfs = pickle.load(f)\n",
    "    \n",
    "df['mfs'] = mfs"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "UnpicklingError",
     "evalue": "pickle data was truncated",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-5e99daae4dc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./audio_features/mfs.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mfs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "with open('./audio_features/chromas.pickle', 'rb') as f:\n",
    "    chromas = pickle.load(f)\n",
    "    \n",
    "df['chromas'] = chromas"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "df['zero_crossings'].isna().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "13994"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# This drops rows without every value (drops rows that don't have lyrics)\n",
    "# This is necessary for doing embeddings\n",
    "\n",
    "# df.dropna(subset=[n for n in df if n != 'isrc'], inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "df = df[(df.year.astype(int) < 2011) & (df.year.astype(int) > 1957)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "df.dropna(subset=['zero_crossings', 'mels'], inplace=True) # Drops all rows that don't have audio features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have our completed dataset."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "df.describe()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>explicit</th>\n",
       "      <th>hit</th>\n",
       "      <th>current_popularity</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>...</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>artist_popularity</th>\n",
       "      <th>artist_num_hits</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>lyric_length</th>\n",
       "      <th>zero_crossings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>2.274000e+04</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>18728.000000</td>\n",
       "      <td>18728.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "      <td>22740.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.050836</td>\n",
       "      <td>0.515611</td>\n",
       "      <td>25.745471</td>\n",
       "      <td>0.560486</td>\n",
       "      <td>0.632077</td>\n",
       "      <td>5.307080</td>\n",
       "      <td>-8.998418</td>\n",
       "      <td>0.692436</td>\n",
       "      <td>0.076460</td>\n",
       "      <td>0.286520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.546271</td>\n",
       "      <td>121.359167</td>\n",
       "      <td>2.362065e+05</td>\n",
       "      <td>3.909719</td>\n",
       "      <td>48.319613</td>\n",
       "      <td>3.879991</td>\n",
       "      <td>0.113119</td>\n",
       "      <td>0.489335</td>\n",
       "      <td>18371.364600</td>\n",
       "      <td>72429.075550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.219667</td>\n",
       "      <td>0.499767</td>\n",
       "      <td>21.227784</td>\n",
       "      <td>0.172875</td>\n",
       "      <td>0.230512</td>\n",
       "      <td>3.577791</td>\n",
       "      <td>4.143219</td>\n",
       "      <td>0.461495</td>\n",
       "      <td>0.093986</td>\n",
       "      <td>0.308756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.258915</td>\n",
       "      <td>28.512912</td>\n",
       "      <td>9.748779e+04</td>\n",
       "      <td>0.389834</td>\n",
       "      <td>21.490694</td>\n",
       "      <td>7.862817</td>\n",
       "      <td>0.185765</td>\n",
       "      <td>0.161196</td>\n",
       "      <td>79332.841236</td>\n",
       "      <td>23596.246983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-52.243000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.152000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>0.469000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-11.311500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>99.723500</td>\n",
       "      <td>1.796270e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428488</td>\n",
       "      <td>609.000000</td>\n",
       "      <td>56113.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.572000</td>\n",
       "      <td>0.656500</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-8.344500</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.043900</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557000</td>\n",
       "      <td>120.029000</td>\n",
       "      <td>2.231935e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.096875</td>\n",
       "      <td>0.500384</td>\n",
       "      <td>1137.000000</td>\n",
       "      <td>71679.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.687000</td>\n",
       "      <td>0.824000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-5.975000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073400</td>\n",
       "      <td>0.518000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>138.068750</td>\n",
       "      <td>2.702032e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.221442</td>\n",
       "      <td>0.575730</td>\n",
       "      <td>2060.000000</td>\n",
       "      <td>87627.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>0.986000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>4.072000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.996000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>233.429000</td>\n",
       "      <td>3.079158e+06</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>824082.000000</td>\n",
       "      <td>305996.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           explicit           hit  current_popularity  danceability  \\\n",
       "count  22740.000000  22740.000000        22740.000000  22740.000000   \n",
       "mean       0.050836      0.515611           25.745471      0.560486   \n",
       "std        0.219667      0.499767           21.227784      0.172875   \n",
       "min        0.000000      0.000000            0.000000      0.000000   \n",
       "25%        0.000000      0.000000            7.000000      0.444000   \n",
       "50%        0.000000      1.000000           21.000000      0.572000   \n",
       "75%        0.000000      1.000000           42.000000      0.687000   \n",
       "max        1.000000      1.000000           85.000000      0.986000   \n",
       "\n",
       "             energy           key      loudness          mode   speechiness  \\\n",
       "count  22740.000000  22740.000000  22740.000000  22740.000000  22740.000000   \n",
       "mean       0.632077      5.307080     -8.998418      0.692436      0.076460   \n",
       "std        0.230512      3.577791      4.143219      0.461495      0.093986   \n",
       "min        0.000026      0.000000    -52.243000      0.000000      0.000000   \n",
       "25%        0.469000      2.000000    -11.311500      0.000000      0.033200   \n",
       "50%        0.656500      5.000000     -8.344500      1.000000      0.043900   \n",
       "75%        0.824000      9.000000     -5.975000      1.000000      0.073400   \n",
       "max        1.000000     11.000000      4.072000      1.000000      0.962000   \n",
       "\n",
       "       acousticness  ...       valence         tempo   duration_ms  \\\n",
       "count  22740.000000  ...  22740.000000  22740.000000  2.274000e+04   \n",
       "mean       0.286520  ...      0.546271    121.359167  2.362065e+05   \n",
       "std        0.308756  ...      0.258915     28.512912  9.748779e+04   \n",
       "min        0.000000  ...      0.000000      0.000000  1.152000e+04   \n",
       "25%        0.018700  ...      0.340000     99.723500  1.796270e+05   \n",
       "50%        0.152000  ...      0.557000    120.029000  2.231935e+05   \n",
       "75%        0.518000  ...      0.764000    138.068750  2.702032e+05   \n",
       "max        0.996000  ...      0.990000    233.429000  3.079158e+06   \n",
       "\n",
       "       time_signature  artist_popularity  artist_num_hits      polarity  \\\n",
       "count    22740.000000       22740.000000     22740.000000  18728.000000   \n",
       "mean         3.909719          48.319613         3.879991      0.113119   \n",
       "std          0.389834          21.490694         7.862817      0.185765   \n",
       "min          0.000000           0.000000         0.000000     -1.000000   \n",
       "25%          4.000000          34.000000         0.000000      0.000000   \n",
       "50%          4.000000          51.000000         1.000000      0.096875   \n",
       "75%          4.000000          65.000000         4.000000      0.221442   \n",
       "max          5.000000         100.000000        93.000000      1.000000   \n",
       "\n",
       "       subjectivity   lyric_length  zero_crossings  \n",
       "count  18728.000000   22740.000000    22740.000000  \n",
       "mean       0.489335   18371.364600    72429.075550  \n",
       "std        0.161196   79332.841236    23596.246983  \n",
       "min        0.000000       0.000000        1.000000  \n",
       "25%        0.428488     609.000000    56113.750000  \n",
       "50%        0.500384    1137.000000    71679.500000  \n",
       "75%        0.575730    2060.000000    87627.000000  \n",
       "max        1.000000  824082.000000   305996.000000  \n",
       "\n",
       "[8 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#%pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "data_df = df.groupby(['year']).mean().reset_index()\n",
    "data_df = data_df[data_df['year'].astype(int) > 1960]\n",
    "sns_plot = sns.regplot(data_df['year'].astype(int), data_df['polarity'], order=1, ci=None)\n",
    "# sns_plot.figure.savefig('polarity.pdf')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation\n",
    "\n",
    "We extract our target variable (hit or not) from the dataset. Then, we drop irrelevant and non-numerical columns from the dataset to form our x inputs.\n",
    "\n",
    "The dataset is then split into training and validation sets. The new datasets are not shuffled to ensure that the lyrics correspond to the correct auxiliary variables."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "source": [
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_data = df['hit']\n",
    "x_data = df.drop(['hit', 'artist', 'artist_id', 'isrc', 'lyrics', 'song', 'id', 'spotify_track_id', 'spotify_track_album', 'analysis_url', 'uri', 'track_href', 'type', 'current_popularity', 'mels', 'chromas', 'zero_crossings'], axis=1, inplace=False)\n",
    "print(x_data.columns)\n",
    "x_data_train, x_data_test, y_data_train, y_data_test = train_test_split(\n",
    "    x_data, y_data, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "lyrics_train, lyrics_eval = train_test_split(df['lyrics'], test_size=0.2, shuffle=False)\n",
    "print(len(y_data_train))\n",
    "print(len(lyrics_train))\n",
    "print(len(x_data_train))\n",
    "print(y_data_train.describe())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['year', 'explicit', 'danceability', 'energy', 'key', 'loudness', 'mode',\n",
      "       'speechiness', 'acousticness', 'instrumentalness', 'liveness',\n",
      "       'valence', 'tempo', 'duration_ms', 'time_signature',\n",
      "       'artist_popularity', 'artist_num_hits', 'polarity', 'subjectivity',\n",
      "       'lyric_length'],\n",
      "      dtype='object')\n",
      "18192\n",
      "18192\n",
      "18192\n",
      "count    18192.000000\n",
      "mean         0.518470\n",
      "std          0.499672\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          1.000000\n",
      "75%          1.000000\n",
      "max          1.000000\n",
      "Name: hit, dtype: float64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "source": [
    "polarity_mean = x_data_train['polarity'].mean()\n",
    "subjectivity_mean = x_data_train['subjectivity'].mean()\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "x_data_train['polarity'] = x_data_train['polarity'].fillna(polarity_mean)\n",
    "x_data_test['polarity'] = x_data_test['polarity'].fillna(polarity_mean)\n",
    "x_data_train['subjectivity'] = x_data_train['subjectivity'].fillna(subjectivity_mean)\n",
    "x_data_test['subjectivity'] = x_data_test['subjectivity'].fillna(subjectivity_mean)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "source": [
    "import numpy as np\n",
    "\n",
    "mels = []\n",
    "\n",
    "for arr in df['mels']:\n",
    "    mels.append(arr.flatten())\n",
    "    \n",
    "mels = np.asarray(mels).astype('float32')\n",
    "\n",
    "\n",
    "chromas = []\n",
    "\n",
    "for arr in df['chromas']:\n",
    "    chromas.append(arr.flatten())\n",
    "    \n",
    "chromas = np.asarray(chromas).astype('float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "source": [
    "mels_train, mels_test, chromas_train, chromas_test = train_test_split(\n",
    "    mels, chromas, test_size=0.2, shuffle=False\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then scale our auxiliary input variables using the StandardScaler."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_model = StandardScaler()\n",
    "scaler_model.fit(x_data_train)\n",
    "\n",
    "x_data_train = pd.DataFrame(\n",
    "    scaler_model.transform(x_data_train),\n",
    "    columns=x_data_train.columns,\n",
    "    index=x_data_train.index\n",
    ")\n",
    "x_data_test = pd.DataFrame(\n",
    "    scaler_model.transform(x_data_test),\n",
    "    columns=x_data_test.columns,\n",
    "    index=x_data_test.index\n",
    ")\n",
    "\n",
    "mel_scaler_model = StandardScaler()\n",
    "mel_scaler_model.fit(mels_train)\n",
    "\n",
    "mels_train = pd.DataFrame(mel_scaler_model.transform(mels_train))\n",
    "mels_test = pd.DataFrame(mel_scaler_model.transform(mels_test))\n",
    "\n",
    "chroma_scaler_model = StandardScaler()\n",
    "chroma_scaler_model.fit(chromas_train)\n",
    "\n",
    "chromas_train = pd.DataFrame(chroma_scaler_model.transform(chromas_train))\n",
    "chromas_test = pd.DataFrame(chroma_scaler_model.transform(chromas_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Classification Models\n",
    "\n",
    "The below code blocks show the results of the classification task using the auxiliary variables alone as inputs into various classifier models."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LR = LogisticRegression(random_state=0).fit(x_data_train, y_data_train)\n",
    "print(round(LR.score(x_data_test, y_data_test), 4))\n",
    "lr_preds = LR.predict(x_data_test)\n",
    "print(sk.metrics.precision_recall_fscore_support(y_data_test, lr_preds, average='binary'))\n",
    "print(sk.metrics.confusion_matrix(y_data_test, lr_preds))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8162\n",
      "(0.8408984557791296, 0.7836894897514174, 0.8112866817155756, None)\n",
      "[[1915  340]\n",
      " [ 496 1797]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "source": [
    "SVM = sk.svm.SVC()\n",
    "SVM.fit(x_data_train, y_data_train)\n",
    "print(round(SVM.score(x_data_test, y_data_test), 4))\n",
    "svm_preds = SVM.predict(x_data_test)\n",
    "print(sk.metrics.precision_recall_fscore_support(y_data_test, svm_preds, average='binary'))\n",
    "print(sk.metrics.confusion_matrix(y_data_test, svm_preds))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8245\n",
      "(0.8417924096936442, 0.8028783253379852, 0.821875, None)\n",
      "[[1909  346]\n",
      " [ 452 1841]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RF = RandomForestClassifier(n_estimators=500, max_depth=100, random_state=0)\n",
    "RF.fit(x_data_train, y_data_train)\n",
    "print(round(RF.score(x_data_test, y_data_test), 4))\n",
    "rf_preds = RF.predict(x_data_test)\n",
    "print(sk.metrics.precision_recall_fscore_support(y_data_test, rf_preds, average='binary'))\n",
    "print(sk.metrics.confusion_matrix(y_data_test, rf_preds))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8533\n",
      "(0.8421717171717171, 0.8726559092891408, 0.8571428571428571, None)\n",
      "[[1880  375]\n",
      " [ 292 2001]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "NN = MLPClassifier(solver='adam', alpha=0.01, hidden_layer_sizes=(10, 10, 10), random_state=1, activation='relu')\n",
    "NN.fit(x_data_train, y_data_train)\n",
    "print(round(NN.score(x_data_test, y_data_test), 4))\n",
    "nn_preds = NN.predict(x_data_test)\n",
    "print(sk.metrics.precision_recall_fscore_support(y_data_test, nn_preds, average='binary'))\n",
    "print(sk.metrics.confusion_matrix(y_data_test, nn_preds))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8404\n",
      "(0.8332624415142492, 0.8543392935019625, 0.8436692506459947, None)\n",
      "[[1863  392]\n",
      " [ 334 1959]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "NB = BernoulliNB()\n",
    "NB.fit(x_data_train, y_data_train)\n",
    "print(round(NB.score(x_data_test, y_data_test), 4))\n",
    "nb_preds = NB.predict(x_data_test)\n",
    "print(sk.metrics.precision_recall_fscore_support(y_data_test, nb_preds, average='binary'))\n",
    "print(sk.metrics.confusion_matrix(y_data_test, nb_preds))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.7887\n",
      "(0.8024523160762943, 0.7706061927605756, 0.786206896551724, None)\n",
      "[[1820  435]\n",
      " [ 526 1767]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from embeddings import EmbeddingGenerator\n",
    "\n",
    "embedding = EmbeddingGenerator(\n",
    "    max_sequence_len=100,\n",
    "    filename='glove.840B.300d.txt'\n",
    ")\n",
    "\n",
    "print('Pretrained embedding loaded')\n",
    "print('Embedding dimension:', embedding.embedding_dimension)\n",
    "\n",
    "embedding.generate_word_index(texts=lyrics_train)  # Fits Tokenizer on words in X training data\n",
    "print('Word index:', len(embedding.word_index))\n",
    "\n",
    "# The below generate_sequences lines converts the words into integers,\n",
    "# using the word indexes from the training data\n",
    "seqs_train = embedding.generate_sequences(lyrics_train)\n",
    "seqs_eval = embedding.generate_sequences(lyrics_eval)\n",
    "\n",
    "print('Sequences created')\n",
    "\n",
    "embedding.generate_embedding_matrix()\n",
    "\n",
    "print('Vocab size:', embedding.vocab_size)\n",
    "print(embedding.embedding_matrix.shape)\n",
    "\n",
    "with open('embedding-audio.pickle', 'wb') as f:\n",
    "    pickle.dump(embedding, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "with open('embedding-audio.pickle', 'rb') as f:\n",
    "    embedding = pickle.load(f)\n",
    "    \n",
    "seqs_train = embedding.generate_sequences(lyrics_train)\n",
    "seqs_eval = embedding.generate_sequences(lyrics_eval)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, Embedding, \\\n",
    "    Input, Flatten, MaxPooling1D, Conv1D, concatenate, LSTM\n",
    "from tensorflow.keras.models import Model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "source": [
    "print('Building model...')\n",
    "\n",
    "lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.00)\n",
    "\n",
    "# Branch 1\n",
    "aux_input = Input(shape=(len(x_data_train.columns),))\n",
    "aux = Dense(400, activation=lrelu)(aux_input)\n",
    "aux = BatchNormalization()(aux)\n",
    "aux = Model(inputs=aux_input, outputs=aux)\n",
    "\n",
    "# Branch 2\n",
    "emb_input = Input(shape=(embedding.max_sequence_len,))\n",
    "emb = Embedding(embedding.vocab_size, embedding.embedding_dimension,\n",
    "                input_length=embedding.max_sequence_len,\n",
    "                weights=[embedding.embedding_matrix],\n",
    "                trainable=True)(emb_input)\n",
    "emb = Conv1D(filters=16, kernel_size=4, activation=lrelu)(emb)\n",
    "emb = LSTM(50)(emb)\n",
    "emb = Flatten()(emb)\n",
    "emb = BatchNormalization()(emb)\n",
    "emb = Dropout(.2)(emb)\n",
    "emb = Dense(200, activation=lrelu)(emb)\n",
    "emb = BatchNormalization()(emb)\n",
    "emb = Dropout(.2)(emb)\n",
    "emb = Dense(200, activation=lrelu)(emb)\n",
    "emb = BatchNormalization()(emb)\n",
    "emb = Dropout(.2)(emb)\n",
    "emb = Dense(200, activation=lrelu)(emb)\n",
    "emb = BatchNormalization()(emb)\n",
    "emb = Dropout(.2)(emb)\n",
    "emb = Dense(200, activation=lrelu)(emb)\n",
    "emb = BatchNormalization()(emb)\n",
    "emb = Dropout(.2)(emb)\n",
    "emb = Dense(200, activation=lrelu)(emb)\n",
    "emb = BatchNormalization()(emb)\n",
    "emb = Dropout(.2)(emb)\n",
    "emb = Dense(200, activation=lrelu)(emb)\n",
    "emb = BatchNormalization()(emb)\n",
    "emb = Model(inputs=emb_input, outputs=emb)\n",
    "\n",
    "# Combined\n",
    "combined = concatenate([aux.output, emb.output])\n",
    "z = Dense(100, activation=lrelu)(combined)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(.2)(z)\n",
    "z = Dense(100, activation=lrelu)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(.2)(z)\n",
    "z = Dense(100, activation=lrelu)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(.2)(z)\n",
    "z = Dense(100, activation=lrelu)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(.2)(z)\n",
    "z = Dense(100, activation=lrelu)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(.2)(z)\n",
    "z = Dense(100)(z)\n",
    "z = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(inputs=[aux.input, emb.input], outputs=z)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(0.01) # Try RMSprop too?\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001, verbose=1)\n",
    "\n",
    "print(x_data_train.shape)\n",
    "print(seqs_train.shape)\n",
    "# print(model.summary())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building model...\n",
      "(18192, 20)\n",
      "(18192, 100)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "source": [
    "history = model.fit(\n",
    "    [x_data_train, seqs_train],\n",
    "    y_data_train,\n",
    "    validation_data=([x_data_test, seqs_eval], y_data_test),\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[es, rlr]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "285/285 [==============================] - 6s 22ms/step - loss: 0.6224 - acc: 0.6660 - precision_48: 0.6781 - recall_48: 0.6771 - val_loss: 0.5697 - val_acc: 0.6966 - val_precision_48: 0.6351 - val_recall_48: 0.9359\n",
      "Epoch 2/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.5163 - acc: 0.7552 - precision_48: 0.7579 - recall_48: 0.7757 - val_loss: 0.4659 - val_acc: 0.7911 - val_precision_48: 0.7664 - val_recall_48: 0.8426\n",
      "Epoch 3/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4914 - acc: 0.7733 - precision_48: 0.7771 - recall_48: 0.7890 - val_loss: 0.4517 - val_acc: 0.7979 - val_precision_48: 0.7642 - val_recall_48: 0.8666\n",
      "Epoch 4/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4705 - acc: 0.7918 - precision_48: 0.7923 - recall_48: 0.8110 - val_loss: 0.4385 - val_acc: 0.8056 - val_precision_48: 0.7997 - val_recall_48: 0.8199\n",
      "Epoch 5/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4627 - acc: 0.7968 - precision_48: 0.7977 - recall_48: 0.8146 - val_loss: 0.4348 - val_acc: 0.8102 - val_precision_48: 0.7895 - val_recall_48: 0.8504\n",
      "Epoch 6/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4504 - acc: 0.8018 - precision_48: 0.8030 - recall_48: 0.8187 - val_loss: 0.4274 - val_acc: 0.8138 - val_precision_48: 0.8002 - val_recall_48: 0.8404\n",
      "Epoch 7/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4442 - acc: 0.8068 - precision_48: 0.8089 - recall_48: 0.8216 - val_loss: 0.4155 - val_acc: 0.8252 - val_precision_48: 0.8134 - val_recall_48: 0.8478\n",
      "Epoch 8/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4356 - acc: 0.8130 - precision_48: 0.8152 - recall_48: 0.8267 - val_loss: 0.4140 - val_acc: 0.8248 - val_precision_48: 0.8002 - val_recall_48: 0.8696\n",
      "Epoch 9/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4301 - acc: 0.8183 - precision_48: 0.8178 - recall_48: 0.8357 - val_loss: 0.4171 - val_acc: 0.8221 - val_precision_48: 0.7860 - val_recall_48: 0.8892\n",
      "Epoch 10/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4255 - acc: 0.8181 - precision_48: 0.8195 - recall_48: 0.8326 - val_loss: 0.4007 - val_acc: 0.8351 - val_precision_48: 0.8200 - val_recall_48: 0.8622\n",
      "Epoch 11/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4176 - acc: 0.8245 - precision_48: 0.8245 - recall_48: 0.8403 - val_loss: 0.4003 - val_acc: 0.8336 - val_precision_48: 0.8155 - val_recall_48: 0.8657\n",
      "Epoch 12/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4120 - acc: 0.8283 - precision_48: 0.8306 - recall_48: 0.8403 - val_loss: 0.4021 - val_acc: 0.8320 - val_precision_48: 0.8016 - val_recall_48: 0.8862\n",
      "Epoch 13/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4101 - acc: 0.8283 - precision_48: 0.8292 - recall_48: 0.8423 - val_loss: 0.4009 - val_acc: 0.8322 - val_precision_48: 0.8021 - val_recall_48: 0.8857\n",
      "Epoch 14/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4033 - acc: 0.8350 - precision_48: 0.8329 - recall_48: 0.8529 - val_loss: 0.3910 - val_acc: 0.8369 - val_precision_48: 0.8211 - val_recall_48: 0.8648\n",
      "Epoch 15/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.4010 - acc: 0.8326 - precision_48: 0.8325 - recall_48: 0.8476 - val_loss: 0.3914 - val_acc: 0.8377 - val_precision_48: 0.8301 - val_recall_48: 0.8526\n",
      "Epoch 16/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3961 - acc: 0.8369 - precision_48: 0.8383 - recall_48: 0.8492 - val_loss: 0.3948 - val_acc: 0.8360 - val_precision_48: 0.8083 - val_recall_48: 0.8844\n",
      "Epoch 17/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3944 - acc: 0.8400 - precision_48: 0.8388 - recall_48: 0.8558 - val_loss: 0.3917 - val_acc: 0.8388 - val_precision_48: 0.8115 - val_recall_48: 0.8862\n",
      "Epoch 18/200\n",
      "285/285 [==============================] - 6s 19ms/step - loss: 0.3933 - acc: 0.8392 - precision_48: 0.8394 - recall_48: 0.8529 - val_loss: 0.3934 - val_acc: 0.8412 - val_precision_48: 0.8116 - val_recall_48: 0.8923\n",
      "Epoch 19/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3880 - acc: 0.8436 - precision_48: 0.8426 - recall_48: 0.8588 - val_loss: 0.3859 - val_acc: 0.8401 - val_precision_48: 0.8304 - val_recall_48: 0.8583\n",
      "Epoch 20/200\n",
      "285/285 [==============================] - 6s 19ms/step - loss: 0.3867 - acc: 0.8438 - precision_48: 0.8432 - recall_48: 0.8584 - val_loss: 0.3896 - val_acc: 0.8347 - val_precision_48: 0.8387 - val_recall_48: 0.8321\n",
      "Epoch 21/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3861 - acc: 0.8469 - precision_48: 0.8467 - recall_48: 0.8605 - val_loss: 0.3907 - val_acc: 0.8397 - val_precision_48: 0.8116 - val_recall_48: 0.8884\n",
      "Epoch 22/200\n",
      "285/285 [==============================] - 6s 19ms/step - loss: 0.3815 - acc: 0.8466 - precision_48: 0.8417 - recall_48: 0.8672 - val_loss: 0.3872 - val_acc: 0.8404 - val_precision_48: 0.8218 - val_recall_48: 0.8727\n",
      "Epoch 23/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3790 - acc: 0.8478 - precision_48: 0.8476 - recall_48: 0.8614 - val_loss: 0.3840 - val_acc: 0.8419 - val_precision_48: 0.8276 - val_recall_48: 0.8670\n",
      "Epoch 24/200\n",
      "285/285 [==============================] - 6s 19ms/step - loss: 0.3770 - acc: 0.8503 - precision_48: 0.8501 - recall_48: 0.8635 - val_loss: 0.3831 - val_acc: 0.8450 - val_precision_48: 0.8207 - val_recall_48: 0.8862\n",
      "Epoch 25/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3789 - acc: 0.8489 - precision_48: 0.8449 - recall_48: 0.8680 - val_loss: 0.3966 - val_acc: 0.8397 - val_precision_48: 0.8103 - val_recall_48: 0.8905\n",
      "Epoch 26/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3786 - acc: 0.8480 - precision_48: 0.8454 - recall_48: 0.8650 - val_loss: 0.3864 - val_acc: 0.8445 - val_precision_48: 0.8144 - val_recall_48: 0.8958\n",
      "Epoch 27/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3752 - acc: 0.8497 - precision_48: 0.8456 - recall_48: 0.8686 - val_loss: 0.3890 - val_acc: 0.8434 - val_precision_48: 0.8113 - val_recall_48: 0.8984\n",
      "Epoch 28/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3741 - acc: 0.8505 - precision_48: 0.8500 - recall_48: 0.8642 - val_loss: 0.3787 - val_acc: 0.8465 - val_precision_48: 0.8313 - val_recall_48: 0.8727\n",
      "Epoch 29/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3768 - acc: 0.8493 - precision_48: 0.8440 - recall_48: 0.8701 - val_loss: 0.3836 - val_acc: 0.8417 - val_precision_48: 0.8193 - val_recall_48: 0.8801\n",
      "Epoch 30/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3729 - acc: 0.8507 - precision_48: 0.8463 - recall_48: 0.8701 - val_loss: 0.3798 - val_acc: 0.8430 - val_precision_48: 0.8245 - val_recall_48: 0.8748\n",
      "Epoch 31/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3722 - acc: 0.8503 - precision_48: 0.8451 - recall_48: 0.8709 - val_loss: 0.3784 - val_acc: 0.8434 - val_precision_48: 0.8215 - val_recall_48: 0.8809\n",
      "Epoch 32/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3656 - acc: 0.8536 - precision_48: 0.8461 - recall_48: 0.8772 - val_loss: 0.3906 - val_acc: 0.8362 - val_precision_48: 0.8009 - val_recall_48: 0.8984\n",
      "Epoch 33/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3677 - acc: 0.8539 - precision_48: 0.8484 - recall_48: 0.8745 - val_loss: 0.3783 - val_acc: 0.8441 - val_precision_48: 0.8347 - val_recall_48: 0.8613\n",
      "Epoch 34/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3647 - acc: 0.8544 - precision_48: 0.8505 - recall_48: 0.8725 - val_loss: 0.3833 - val_acc: 0.8412 - val_precision_48: 0.8123 - val_recall_48: 0.8910\n",
      "Epoch 35/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3651 - acc: 0.8552 - precision_48: 0.8501 - recall_48: 0.8749 - val_loss: 0.3830 - val_acc: 0.8421 - val_precision_48: 0.8124 - val_recall_48: 0.8932\n",
      "Epoch 36/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3645 - acc: 0.8561 - precision_48: 0.8494 - recall_48: 0.8783 - val_loss: 0.3792 - val_acc: 0.8419 - val_precision_48: 0.8189 - val_recall_48: 0.8814\n",
      "Epoch 37/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3629 - acc: 0.8567 - precision_48: 0.8500 - recall_48: 0.8787 - val_loss: 0.3859 - val_acc: 0.8412 - val_precision_48: 0.8074 - val_recall_48: 0.8997\n",
      "Epoch 38/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3643 - acc: 0.8526 - precision_48: 0.8454 - recall_48: 0.8758 - val_loss: 0.3806 - val_acc: 0.8441 - val_precision_48: 0.8204 - val_recall_48: 0.8844\n",
      "Epoch 39/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3591 - acc: 0.8566 - precision_48: 0.8492 - recall_48: 0.8796 - val_loss: 0.3841 - val_acc: 0.8441 - val_precision_48: 0.8188 - val_recall_48: 0.8870\n",
      "Epoch 40/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3612 - acc: 0.8555 - precision_48: 0.8507 - recall_48: 0.8749 - val_loss: 0.3842 - val_acc: 0.8412 - val_precision_48: 0.8118 - val_recall_48: 0.8918\n",
      "Epoch 41/200\n",
      "283/285 [============================>.] - ETA: 0s - loss: 0.3593 - acc: 0.8580 - precision_48: 0.8486 - recall_48: 0.8836\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3591 - acc: 0.8583 - precision_48: 0.8492 - recall_48: 0.8836 - val_loss: 0.3808 - val_acc: 0.8428 - val_precision_48: 0.8205 - val_recall_48: 0.8809\n",
      "Epoch 42/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3495 - acc: 0.8637 - precision_48: 0.8540 - recall_48: 0.8892 - val_loss: 0.3768 - val_acc: 0.8439 - val_precision_48: 0.8219 - val_recall_48: 0.8814\n",
      "Epoch 43/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3554 - acc: 0.8605 - precision_48: 0.8530 - recall_48: 0.8832 - val_loss: 0.3757 - val_acc: 0.8443 - val_precision_48: 0.8312 - val_recall_48: 0.8674\n",
      "Epoch 44/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3505 - acc: 0.8621 - precision_48: 0.8536 - recall_48: 0.8860 - val_loss: 0.3757 - val_acc: 0.8474 - val_precision_48: 0.8262 - val_recall_48: 0.8831\n",
      "Epoch 45/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3522 - acc: 0.8585 - precision_48: 0.8504 - recall_48: 0.8822 - val_loss: 0.3761 - val_acc: 0.8463 - val_precision_48: 0.8203 - val_recall_48: 0.8901\n",
      "Epoch 46/200\n",
      "285/285 [==============================] - 6s 19ms/step - loss: 0.3513 - acc: 0.8614 - precision_48: 0.8531 - recall_48: 0.8851 - val_loss: 0.3783 - val_acc: 0.8456 - val_precision_48: 0.8166 - val_recall_48: 0.8949\n",
      "Epoch 47/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3508 - acc: 0.8621 - precision_48: 0.8515 - recall_48: 0.8891 - val_loss: 0.3757 - val_acc: 0.8456 - val_precision_48: 0.8275 - val_recall_48: 0.8766\n",
      "Epoch 48/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3493 - acc: 0.8618 - precision_48: 0.8529 - recall_48: 0.8861 - val_loss: 0.3755 - val_acc: 0.8467 - val_precision_48: 0.8223 - val_recall_48: 0.8879\n",
      "Epoch 49/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3520 - acc: 0.8618 - precision_48: 0.8510 - recall_48: 0.8891 - val_loss: 0.3746 - val_acc: 0.8470 - val_precision_48: 0.8298 - val_recall_48: 0.8761\n",
      "Epoch 50/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3472 - acc: 0.8643 - precision_48: 0.8544 - recall_48: 0.8899 - val_loss: 0.3759 - val_acc: 0.8467 - val_precision_48: 0.8254 - val_recall_48: 0.8827\n",
      "Epoch 51/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3479 - acc: 0.8633 - precision_48: 0.8555 - recall_48: 0.8859 - val_loss: 0.3774 - val_acc: 0.8430 - val_precision_48: 0.8285 - val_recall_48: 0.8683\n",
      "Epoch 52/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3505 - acc: 0.8626 - precision_48: 0.8529 - recall_48: 0.8883 - val_loss: 0.3742 - val_acc: 0.8478 - val_precision_48: 0.8250 - val_recall_48: 0.8862\n",
      "Epoch 53/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3467 - acc: 0.8642 - precision_48: 0.8555 - recall_48: 0.8880 - val_loss: 0.3749 - val_acc: 0.8485 - val_precision_48: 0.8292 - val_recall_48: 0.8809\n",
      "Epoch 54/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3497 - acc: 0.8620 - precision_48: 0.8532 - recall_48: 0.8865 - val_loss: 0.3796 - val_acc: 0.8448 - val_precision_48: 0.8130 - val_recall_48: 0.8988\n",
      "Epoch 55/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3496 - acc: 0.8635 - precision_48: 0.8547 - recall_48: 0.8876 - val_loss: 0.3736 - val_acc: 0.8467 - val_precision_48: 0.8281 - val_recall_48: 0.8783\n",
      "Epoch 56/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3458 - acc: 0.8629 - precision_48: 0.8538 - recall_48: 0.8875 - val_loss: 0.3744 - val_acc: 0.8474 - val_precision_48: 0.8262 - val_recall_48: 0.8831\n",
      "Epoch 57/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3459 - acc: 0.8658 - precision_48: 0.8561 - recall_48: 0.8908 - val_loss: 0.3759 - val_acc: 0.8459 - val_precision_48: 0.8215 - val_recall_48: 0.8870\n",
      "Epoch 58/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3432 - acc: 0.8644 - precision_48: 0.8539 - recall_48: 0.8910 - val_loss: 0.3749 - val_acc: 0.8478 - val_precision_48: 0.8237 - val_recall_48: 0.8884\n",
      "Epoch 59/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3452 - acc: 0.8638 - precision_48: 0.8543 - recall_48: 0.8889 - val_loss: 0.3767 - val_acc: 0.8459 - val_precision_48: 0.8176 - val_recall_48: 0.8936\n",
      "Epoch 60/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3439 - acc: 0.8668 - precision_48: 0.8572 - recall_48: 0.8915 - val_loss: 0.3748 - val_acc: 0.8474 - val_precision_48: 0.8267 - val_recall_48: 0.8823\n",
      "Epoch 61/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3440 - acc: 0.8666 - precision_48: 0.8590 - recall_48: 0.8887 - val_loss: 0.3754 - val_acc: 0.8463 - val_precision_48: 0.8250 - val_recall_48: 0.8823\n",
      "Epoch 62/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3421 - acc: 0.8680 - precision_48: 0.8601 - recall_48: 0.8902 - val_loss: 0.3755 - val_acc: 0.8461 - val_precision_48: 0.8263 - val_recall_48: 0.8796\n",
      "Epoch 63/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3444 - acc: 0.8655 - precision_48: 0.8561 - recall_48: 0.8903 - val_loss: 0.3742 - val_acc: 0.8459 - val_precision_48: 0.8199 - val_recall_48: 0.8897\n",
      "Epoch 64/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3451 - acc: 0.8637 - precision_48: 0.8560 - recall_48: 0.8861 - val_loss: 0.3745 - val_acc: 0.8478 - val_precision_48: 0.8279 - val_recall_48: 0.8814\n",
      "Epoch 65/200\n",
      "283/285 [============================>.] - ETA: 0s - loss: 0.3442 - acc: 0.8648 - precision_48: 0.8530 - recall_48: 0.8931\n",
      "Epoch 00065: ReduceLROnPlateau reducing learning rate to 0.0003999999724328518.\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3439 - acc: 0.8651 - precision_48: 0.8533 - recall_48: 0.8933 - val_loss: 0.3749 - val_acc: 0.8459 - val_precision_48: 0.8306 - val_recall_48: 0.8722\n",
      "Epoch 66/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3438 - acc: 0.8652 - precision_48: 0.8561 - recall_48: 0.8894 - val_loss: 0.3741 - val_acc: 0.8494 - val_precision_48: 0.8260 - val_recall_48: 0.8884\n",
      "Epoch 67/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3405 - acc: 0.8669 - precision_48: 0.8570 - recall_48: 0.8921 - val_loss: 0.3742 - val_acc: 0.8498 - val_precision_48: 0.8267 - val_recall_48: 0.8884\n",
      "Epoch 68/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3425 - acc: 0.8654 - precision_48: 0.8550 - recall_48: 0.8918 - val_loss: 0.3740 - val_acc: 0.8498 - val_precision_48: 0.8264 - val_recall_48: 0.8888\n",
      "Epoch 69/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3439 - acc: 0.8652 - precision_48: 0.8561 - recall_48: 0.8894 - val_loss: 0.3744 - val_acc: 0.8489 - val_precision_48: 0.8220 - val_recall_48: 0.8940\n",
      "Epoch 70/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3416 - acc: 0.8646 - precision_48: 0.8544 - recall_48: 0.8906 - val_loss: 0.3736 - val_acc: 0.8483 - val_precision_48: 0.8241 - val_recall_48: 0.8888\n",
      "Epoch 71/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3411 - acc: 0.8681 - precision_48: 0.8569 - recall_48: 0.8951 - val_loss: 0.3747 - val_acc: 0.8478 - val_precision_48: 0.8224 - val_recall_48: 0.8905\n",
      "Epoch 72/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3434 - acc: 0.8666 - precision_48: 0.8553 - recall_48: 0.8940 - val_loss: 0.3741 - val_acc: 0.8483 - val_precision_48: 0.8220 - val_recall_48: 0.8923\n",
      "Epoch 73/200\n",
      "285/285 [==============================] - 6s 19ms/step - loss: 0.3394 - acc: 0.8683 - precision_48: 0.8561 - recall_48: 0.8967 - val_loss: 0.3736 - val_acc: 0.8478 - val_precision_48: 0.8255 - val_recall_48: 0.8853\n",
      "Epoch 74/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3445 - acc: 0.8641 - precision_48: 0.8529 - recall_48: 0.8916 - val_loss: 0.3731 - val_acc: 0.8483 - val_precision_48: 0.8265 - val_recall_48: 0.8849\n",
      "Epoch 75/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3430 - acc: 0.8672 - precision_48: 0.8580 - recall_48: 0.8914 - val_loss: 0.3731 - val_acc: 0.8494 - val_precision_48: 0.8279 - val_recall_48: 0.8853\n",
      "Epoch 76/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3427 - acc: 0.8663 - precision_48: 0.8555 - recall_48: 0.8929 - val_loss: 0.3732 - val_acc: 0.8481 - val_precision_48: 0.8264 - val_recall_48: 0.8844\n",
      "Epoch 77/200\n",
      "285/285 [==============================] - 6s 19ms/step - loss: 0.3466 - acc: 0.8629 - precision_48: 0.8515 - recall_48: 0.8910 - val_loss: 0.3729 - val_acc: 0.8492 - val_precision_48: 0.8270 - val_recall_48: 0.8862\n",
      "Epoch 78/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3422 - acc: 0.8675 - precision_48: 0.8569 - recall_48: 0.8936 - val_loss: 0.3739 - val_acc: 0.8494 - val_precision_48: 0.8232 - val_recall_48: 0.8932\n",
      "Epoch 79/200\n",
      "285/285 [==============================] - 6s 19ms/step - loss: 0.3423 - acc: 0.8642 - precision_48: 0.8548 - recall_48: 0.8891 - val_loss: 0.3735 - val_acc: 0.8483 - val_precision_48: 0.8257 - val_recall_48: 0.8862\n",
      "Epoch 80/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3393 - acc: 0.8671 - precision_48: 0.8569 - recall_48: 0.8928 - val_loss: 0.3738 - val_acc: 0.8483 - val_precision_48: 0.8233 - val_recall_48: 0.8901\n",
      "Epoch 81/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3422 - acc: 0.8667 - precision_48: 0.8565 - recall_48: 0.8924 - val_loss: 0.3738 - val_acc: 0.8474 - val_precision_48: 0.8249 - val_recall_48: 0.8853\n",
      "Epoch 82/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3449 - acc: 0.8654 - precision_48: 0.8547 - recall_48: 0.8920 - val_loss: 0.3728 - val_acc: 0.8481 - val_precision_48: 0.8248 - val_recall_48: 0.8870\n",
      "Epoch 83/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3385 - acc: 0.8674 - precision_48: 0.8572 - recall_48: 0.8929 - val_loss: 0.3741 - val_acc: 0.8483 - val_precision_48: 0.8231 - val_recall_48: 0.8905\n",
      "Epoch 84/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3408 - acc: 0.8680 - precision_48: 0.8572 - recall_48: 0.8945 - val_loss: 0.3740 - val_acc: 0.8485 - val_precision_48: 0.8234 - val_recall_48: 0.8905\n",
      "Epoch 85/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3407 - acc: 0.8670 - precision_48: 0.8557 - recall_48: 0.8942 - val_loss: 0.3738 - val_acc: 0.8481 - val_precision_48: 0.8222 - val_recall_48: 0.8914\n",
      "Epoch 86/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3406 - acc: 0.8647 - precision_48: 0.8538 - recall_48: 0.8918 - val_loss: 0.3734 - val_acc: 0.8485 - val_precision_48: 0.8279 - val_recall_48: 0.8831\n",
      "Epoch 87/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3410 - acc: 0.8653 - precision_48: 0.8538 - recall_48: 0.8931 - val_loss: 0.3725 - val_acc: 0.8485 - val_precision_48: 0.8276 - val_recall_48: 0.8836\n",
      "Epoch 88/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3421 - acc: 0.8643 - precision_48: 0.8533 - recall_48: 0.8914 - val_loss: 0.3732 - val_acc: 0.8487 - val_precision_48: 0.8253 - val_recall_48: 0.8879\n",
      "Epoch 89/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3416 - acc: 0.8664 - precision_48: 0.8566 - recall_48: 0.8915 - val_loss: 0.3732 - val_acc: 0.8489 - val_precision_48: 0.8246 - val_recall_48: 0.8897\n",
      "Epoch 90/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3392 - acc: 0.8679 - precision_48: 0.8561 - recall_48: 0.8957 - val_loss: 0.3733 - val_acc: 0.8487 - val_precision_48: 0.8242 - val_recall_48: 0.8897\n",
      "Epoch 91/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3407 - acc: 0.8668 - precision_48: 0.8564 - recall_48: 0.8927 - val_loss: 0.3738 - val_acc: 0.8485 - val_precision_48: 0.8276 - val_recall_48: 0.8836\n",
      "Epoch 92/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3369 - acc: 0.8676 - precision_48: 0.8566 - recall_48: 0.8943 - val_loss: 0.3744 - val_acc: 0.8481 - val_precision_48: 0.8264 - val_recall_48: 0.8844\n",
      "Epoch 93/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3407 - acc: 0.8680 - precision_48: 0.8574 - recall_48: 0.8942 - val_loss: 0.3733 - val_acc: 0.8483 - val_precision_48: 0.8257 - val_recall_48: 0.8862\n",
      "Epoch 94/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3399 - acc: 0.8680 - precision_48: 0.8563 - recall_48: 0.8956 - val_loss: 0.3738 - val_acc: 0.8483 - val_precision_48: 0.8249 - val_recall_48: 0.8875\n",
      "Epoch 95/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3439 - acc: 0.8632 - precision_48: 0.8517 - recall_48: 0.8914 - val_loss: 0.3731 - val_acc: 0.8489 - val_precision_48: 0.8262 - val_recall_48: 0.8870\n",
      "Epoch 96/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3386 - acc: 0.8680 - precision_48: 0.8571 - recall_48: 0.8944 - val_loss: 0.3734 - val_acc: 0.8485 - val_precision_48: 0.8242 - val_recall_48: 0.8892\n",
      "Epoch 97/200\n",
      "283/285 [============================>.] - ETA: 0s - loss: 0.3400 - acc: 0.8660 - precision_48: 0.8557 - recall_48: 0.8919\n",
      "Epoch 00097: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3396 - acc: 0.8662 - precision_48: 0.8562 - recall_48: 0.8918 - val_loss: 0.3735 - val_acc: 0.8496 - val_precision_48: 0.8232 - val_recall_48: 0.8936\n",
      "Epoch 98/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3406 - acc: 0.8684 - precision_48: 0.8572 - recall_48: 0.8954 - val_loss: 0.3728 - val_acc: 0.8487 - val_precision_48: 0.8272 - val_recall_48: 0.8849\n",
      "Epoch 99/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3390 - acc: 0.8686 - precision_48: 0.8562 - recall_48: 0.8974 - val_loss: 0.3732 - val_acc: 0.8485 - val_precision_48: 0.8265 - val_recall_48: 0.8853\n",
      "Epoch 100/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3405 - acc: 0.8665 - precision_48: 0.8553 - recall_48: 0.8937 - val_loss: 0.3730 - val_acc: 0.8483 - val_precision_48: 0.8241 - val_recall_48: 0.8888\n",
      "Epoch 101/200\n",
      "285/285 [==============================] - 6s 19ms/step - loss: 0.3425 - acc: 0.8659 - precision_48: 0.8535 - recall_48: 0.8949 - val_loss: 0.3728 - val_acc: 0.8487 - val_precision_48: 0.8274 - val_recall_48: 0.8844\n",
      "Epoch 102/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3425 - acc: 0.8647 - precision_48: 0.8545 - recall_48: 0.8907 - val_loss: 0.3722 - val_acc: 0.8485 - val_precision_48: 0.8273 - val_recall_48: 0.8840\n",
      "Epoch 103/200\n",
      "285/285 [==============================] - 6s 19ms/step - loss: 0.3414 - acc: 0.8652 - precision_48: 0.8543 - recall_48: 0.8922 - val_loss: 0.3724 - val_acc: 0.8483 - val_precision_48: 0.8270 - val_recall_48: 0.8840\n",
      "Epoch 104/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3361 - acc: 0.8676 - precision_48: 0.8555 - recall_48: 0.8960 - val_loss: 0.3730 - val_acc: 0.8485 - val_precision_48: 0.8247 - val_recall_48: 0.8884\n",
      "Epoch 105/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3419 - acc: 0.8658 - precision_48: 0.8548 - recall_48: 0.8929 - val_loss: 0.3730 - val_acc: 0.8485 - val_precision_48: 0.8263 - val_recall_48: 0.8857\n",
      "Epoch 106/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3397 - acc: 0.8673 - precision_48: 0.8578 - recall_48: 0.8920 - val_loss: 0.3737 - val_acc: 0.8478 - val_precision_48: 0.8224 - val_recall_48: 0.8905\n",
      "Epoch 107/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3393 - acc: 0.8664 - precision_48: 0.8540 - recall_48: 0.8955 - val_loss: 0.3737 - val_acc: 0.8487 - val_precision_48: 0.8227 - val_recall_48: 0.8923\n",
      "Epoch 108/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3388 - acc: 0.8688 - precision_48: 0.8579 - recall_48: 0.8954 - val_loss: 0.3731 - val_acc: 0.8481 - val_precision_48: 0.8248 - val_recall_48: 0.8870\n",
      "Epoch 109/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3370 - acc: 0.8675 - precision_48: 0.8564 - recall_48: 0.8944 - val_loss: 0.3730 - val_acc: 0.8481 - val_precision_48: 0.8261 - val_recall_48: 0.8849\n",
      "Epoch 110/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3397 - acc: 0.8677 - precision_48: 0.8564 - recall_48: 0.8949 - val_loss: 0.3730 - val_acc: 0.8483 - val_precision_48: 0.8249 - val_recall_48: 0.8875\n",
      "Epoch 111/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3390 - acc: 0.8678 - precision_48: 0.8562 - recall_48: 0.8954 - val_loss: 0.3729 - val_acc: 0.8483 - val_precision_48: 0.8249 - val_recall_48: 0.8875\n",
      "Epoch 112/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3377 - acc: 0.8695 - precision_48: 0.8568 - recall_48: 0.8985 - val_loss: 0.3729 - val_acc: 0.8481 - val_precision_48: 0.8256 - val_recall_48: 0.8857\n",
      "Epoch 113/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3396 - acc: 0.8661 - precision_48: 0.8540 - recall_48: 0.8947 - val_loss: 0.3728 - val_acc: 0.8487 - val_precision_48: 0.8261 - val_recall_48: 0.8866\n",
      "Epoch 114/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3417 - acc: 0.8649 - precision_48: 0.8536 - recall_48: 0.8926 - val_loss: 0.3726 - val_acc: 0.8492 - val_precision_48: 0.8276 - val_recall_48: 0.8853\n",
      "Epoch 115/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3409 - acc: 0.8670 - precision_48: 0.8562 - recall_48: 0.8934 - val_loss: 0.3731 - val_acc: 0.8487 - val_precision_48: 0.8256 - val_recall_48: 0.8875\n",
      "Epoch 116/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3394 - acc: 0.8681 - precision_48: 0.8578 - recall_48: 0.8938 - val_loss: 0.3732 - val_acc: 0.8483 - val_precision_48: 0.8233 - val_recall_48: 0.8901\n",
      "Epoch 117/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3439 - acc: 0.8659 - precision_48: 0.8563 - recall_48: 0.8908 - val_loss: 0.3730 - val_acc: 0.8487 - val_precision_48: 0.8245 - val_recall_48: 0.8892\n",
      "Epoch 118/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3405 - acc: 0.8679 - precision_48: 0.8568 - recall_48: 0.8947 - val_loss: 0.3732 - val_acc: 0.8494 - val_precision_48: 0.8266 - val_recall_48: 0.8875\n",
      "Epoch 119/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3408 - acc: 0.8659 - precision_48: 0.8555 - recall_48: 0.8920 - val_loss: 0.3732 - val_acc: 0.8492 - val_precision_48: 0.8249 - val_recall_48: 0.8897\n",
      "Epoch 120/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3403 - acc: 0.8660 - precision_48: 0.8548 - recall_48: 0.8933 - val_loss: 0.3734 - val_acc: 0.8485 - val_precision_48: 0.8239 - val_recall_48: 0.8897\n",
      "Epoch 121/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3397 - acc: 0.8665 - precision_48: 0.8566 - recall_48: 0.8918 - val_loss: 0.3736 - val_acc: 0.8485 - val_precision_48: 0.8236 - val_recall_48: 0.8901\n",
      "Epoch 122/200\n",
      "285/285 [==============================] - 5s 19ms/step - loss: 0.3421 - acc: 0.8652 - precision_48: 0.8541 - recall_48: 0.8925 - val_loss: 0.3730 - val_acc: 0.8481 - val_precision_48: 0.8253 - val_recall_48: 0.8862\n",
      "Epoch 00122: early stopping\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "source": [
    "print('Building model...')\n",
    "\n",
    "lrelu = lambda x: tf.keras.activations.relu(x, alpha=0.00)\n",
    "\n",
    "mels_input = Input(shape=(128, 1))\n",
    "# mels = Conv1D(filters=64, kernel_size=4, activation=lrelu)(mels_input)\n",
    "mels = LSTM(10)(mels_input)\n",
    "mels = Model(inputs=mels_input, outputs=mels)\n",
    "\n",
    "chromas_input = Input(shape=(12, 1))\n",
    "# chromas = Conv1D(filters=64, kernel_size=2, activation=lrelu)(chromas_input)\n",
    "chromas = LSTM(10)(chromas_input)\n",
    "chromas = Model(inputs=chromas_input, outputs=chromas)\n",
    "\n",
    "# Combined\n",
    "z = concatenate([mels.output, chromas.output])\n",
    "z = Dense(200, activation=lrelu)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(.2)(z)\n",
    "z = Dense(200, activation=lrelu)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(.2)(z)\n",
    "z = Dense(200, activation=lrelu)(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = Dropout(.2)(z)\n",
    "z = Dense(200)(z)\n",
    "z = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(inputs=[mels.input, chromas.input], outputs=z)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(0.01) # Try RMSprop too?\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0001, verbose=1)\n",
    "\n",
    "# print(model.summary())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Building model...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "source": [
    "history = model.fit(\n",
    "    [mels_train, chromas_train],\n",
    "    y_data_train,\n",
    "    validation_data=([mels_test, chromas_test], y_data_test),\n",
    "    batch_size=64,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    verbose=1,\n",
    "    callbacks=[es, rlr]\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/200\n",
      "285/285 [==============================] - 4s 13ms/step - loss: 0.7501 - acc: 0.5063 - precision_46: 0.5231 - recall_46: 0.5411 - val_loss: 0.6960 - val_acc: 0.5042 - val_precision_46: 0.5042 - val_recall_46: 1.0000\n",
      "Epoch 2/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.7074 - acc: 0.5022 - precision_46: 0.5172 - recall_46: 0.5994 - val_loss: 0.6974 - val_acc: 0.5079 - val_precision_46: 0.5063 - val_recall_46: 0.9690\n",
      "Epoch 3/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.7015 - acc: 0.4999 - precision_46: 0.5144 - recall_46: 0.6344 - val_loss: 0.6962 - val_acc: 0.5090 - val_precision_46: 0.5089 - val_recall_46: 0.7479\n",
      "Epoch 4/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6989 - acc: 0.5063 - precision_46: 0.5185 - recall_46: 0.6685 - val_loss: 0.6996 - val_acc: 0.5092 - val_precision_46: 0.5129 - val_recall_46: 0.5277\n",
      "Epoch 5/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6987 - acc: 0.5040 - precision_46: 0.5165 - recall_46: 0.6755 - val_loss: 0.6955 - val_acc: 0.5013 - val_precision_46: 0.5034 - val_recall_46: 0.8072\n",
      "Epoch 6/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6972 - acc: 0.5078 - precision_46: 0.5193 - recall_46: 0.6797 - val_loss: 0.6981 - val_acc: 0.4985 - val_precision_46: 0.5024 - val_recall_46: 0.5539\n",
      "Epoch 7/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6978 - acc: 0.5026 - precision_46: 0.5154 - recall_46: 0.6786 - val_loss: 0.7034 - val_acc: 0.5018 - val_precision_46: 0.5042 - val_recall_46: 0.7148\n",
      "Epoch 8/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6968 - acc: 0.5093 - precision_46: 0.5205 - recall_46: 0.6823 - val_loss: 0.6970 - val_acc: 0.5002 - val_precision_46: 0.5070 - val_recall_46: 0.3171\n",
      "Epoch 9/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6978 - acc: 0.5069 - precision_46: 0.5190 - recall_46: 0.6672 - val_loss: 0.6968 - val_acc: 0.5053 - val_precision_46: 0.5051 - val_recall_46: 0.9263\n",
      "Epoch 10/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6973 - acc: 0.5042 - precision_46: 0.5161 - recall_46: 0.7009 - val_loss: 0.7033 - val_acc: 0.5035 - val_precision_46: 0.5049 - val_recall_46: 0.7859\n",
      "Epoch 11/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6966 - acc: 0.5103 - precision_46: 0.5215 - recall_46: 0.6728 - val_loss: 0.6965 - val_acc: 0.4952 - val_precision_46: 0.4994 - val_recall_46: 0.5822\n",
      "Epoch 12/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6978 - acc: 0.5035 - precision_46: 0.5157 - recall_46: 0.6979 - val_loss: 0.6958 - val_acc: 0.5040 - val_precision_46: 0.5044 - val_recall_46: 0.9211\n",
      "Epoch 13/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6967 - acc: 0.5079 - precision_46: 0.5187 - recall_46: 0.7046 - val_loss: 0.6976 - val_acc: 0.5048 - val_precision_46: 0.5045 - val_recall_46: 0.9991\n",
      "Epoch 14/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6966 - acc: 0.5104 - precision_46: 0.5209 - recall_46: 0.6959 - val_loss: 0.6976 - val_acc: 0.5125 - val_precision_46: 0.5140 - val_recall_46: 0.6106\n",
      "Epoch 15/200\n",
      "283/285 [============================>.] - ETA: 0s - loss: 0.6968 - acc: 0.5076 - precision_46: 0.5187 - recall_46: 0.6916\n",
      "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.0019999999552965165.\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6968 - acc: 0.5074 - precision_46: 0.5187 - recall_46: 0.6914 - val_loss: 0.6973 - val_acc: 0.5051 - val_precision_46: 0.5057 - val_recall_46: 0.8168\n",
      "Epoch 16/200\n",
      "285/285 [==============================] - 3s 10ms/step - loss: 0.6954 - acc: 0.5030 - precision_46: 0.5148 - recall_46: 0.7197 - val_loss: 0.6935 - val_acc: 0.5024 - val_precision_46: 0.5034 - val_recall_46: 0.9708\n",
      "Epoch 17/200\n",
      "253/285 [=========================>....] - ETA: 0s - loss: 0.6937 - acc: 0.5119 - precision_46: 0.5190 - recall_46: 0.7506"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-472a1b28c308>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1124\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1125\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1126\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1127\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred_train = model.evaluate([x_data_train, seqs_train], y_data_train)\n",
    "pred_test = model.evaluate([x_data_test, seqs_eval], y_data_test)\n",
    "y_pred_train = model.predict([x_data_train, seqs_train]).astype('int').flatten()\n",
    "y_pred = model.predict([x_data_test, seqs_eval]).astype('int').flatten()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#%pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# %pip install seaborn\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "df_corr = pd.DataFrame() # Correlation matrix\n",
    "df_p = pd.DataFrame()  # Matrix of p-values\n",
    "for x in x_data.columns:\n",
    "    # for y in full_df.columns:\n",
    "    corr = pearsonr(x_data[x].astype(float), y_data)\n",
    "    df_corr.loc[x,0] = corr[0]\n",
    "    df_p.loc[x,0] = corr[1]\n",
    "\n",
    "\n",
    "# mask = np.triu(np.ones_like(df_corr, dtype=np.bool))\n",
    "cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "\n",
    "f = plt.figure(figsize=(8, 7))\n",
    "# plt.rc('xtick', labelsize=10)\n",
    "# plt.rc('ytick', labelsize=10)\n",
    "ax = sns.heatmap(df_corr, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .6})\n",
    "ax.set_title('Pearson Correlation Matrix')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "f = plt.figure(figsize=(8, 7))\n",
    "# plt.rc('xtick', labelsize=10)\n",
    "# plt.rc('ytick', labelsize=10)\n",
    "ax = sns.heatmap(df_p, cmap=cmap, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .6})\n",
    "ax.set_title('P-Value Matrix from Pearson Correlation')\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-2:429704687514:image/tensorflow-2.3-gpu-py37-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}